{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "7ae916fb-47de-4747-9c06-6e985dac5da5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "6f702e7e-737e-499f-997c-e2dff98f6865",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_wins(tensor):\n",
    "    zeros = torch.zeros_like(tensor)\n",
    "    argmax = tensor.argmax(1).reshape(-1, 1)\n",
    "    zeros.scatter_(1, argmax, 1)\n",
    "    wins = zeros.sum(dim=0)\n",
    "    return wins\n",
    "\n",
    "def string_to_tensor(tensor_string, softmax=False):\n",
    "    # Extracts tensor values from string and returns a PyTorch tensor\n",
    "    tensor_data = eval(tensor_string.split('tensor(')[1].split(', device')[0])\n",
    "    t = torch.tensor(tensor_data)[:,:]\n",
    "    # for S3 uncomment the line below and one above\n",
    "    # t = torch.tensor(tensor_data)[:,1:]\n",
    "    softmax_t = F.softmax(t, dim=1)\n",
    "    wins = compute_wins(softmax_t)\n",
    "\n",
    "    return (softmax_t if softmax else t, wins)\n",
    "\n",
    "def extract_last_two_tensors(file_path, softmax=True):\n",
    "    try:\n",
    "        with open(file_path, 'r') as file:\n",
    "            content = file.read()\n",
    "    except Exception as e:\n",
    "        return str(e), None, None\n",
    "\n",
    "    last_delimiter_index = content.rfind('<<<--->>>')\n",
    "    if last_delimiter_index == -1:\n",
    "        return f\"No delimiter found in {file_path}\", None, None\n",
    "\n",
    "    last_section = content[last_delimiter_index + len('<<<--->>>'):].strip()\n",
    "    lines = last_section.splitlines()\n",
    "    tensors = []\n",
    "    tensor_data = []\n",
    "    capturing_tensor = False\n",
    "\n",
    "    for line in lines:\n",
    "        if 'tensor([' in line:\n",
    "            capturing_tensor = True\n",
    "            tensor_data.append(line.strip())\n",
    "        elif capturing_tensor:\n",
    "            tensor_data.append(line.strip())\n",
    "            if 'requires_grad=True' in line:\n",
    "                tensor_string = '\\n'.join(tensor_data)\n",
    "                tensor_obj = string_to_tensor(tensor_string, softmax=softmax)\n",
    "                tensors.append(tensor_obj)\n",
    "                tensor_data = []\n",
    "                capturing_tensor = False\n",
    "\n",
    "    if len(tensors) >= 2:\n",
    "        return None, tensors[-2], tensors[-1]\n",
    "    elif len(tensors) == 1:\n",
    "        return f\"Only one tensor found in {file_path}\", tensors[-1], None\n",
    "    else:\n",
    "        return f\"No tensors found after the delimiter in {file_path}\", None, None\n",
    "\n",
    "def process_directory(directory_path):\n",
    "    tensor_outputs = {}\n",
    "    for filename in os.listdir(directory_path):\n",
    "        if filename.endswith(\".log\"):  # Adjust the extension based on your files\n",
    "            file_path = os.path.join(directory_path, filename)\n",
    "            error, tensor1, tensor2 = extract_last_two_tensors(file_path)\n",
    "            tensor_outputs[filename] = {\"error\": error, \"normal\": tensor1, \"reduction\": tensor2}\n",
    "    return tensor_outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "90311356-20bb-40c2-864f-2a69a3293c3d",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "all_tensors = process_directory(\"logs/rdarts\")\n",
    "all_tensors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3916d473-91ca-4274-a677-400da59940d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "{k:v for k,v in all_tensors.items() if \"_s2_\" in k and \"lora\" in k}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "9507bf56-f5c0-42b2-ac7e-53be6e233bf3",
   "metadata": {},
   "outputs": [],
   "source": [
    "darts_dict = {k:v for k,v in all_tensors.items() if \"_s4_\" in k and not \"lora\" in k}\n",
    "lora_darts_dict = {k:v for k,v in all_tensors.items() if \"_s4_\" in k and \"lora\" in k}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def _get_count_list(count_dict):\n",
    "    normal_count_list = []\n",
    "    reduce_count_list = []\n",
    "    for _, val in count_dict.items():\n",
    "        normal_count_list.append(val[\"normal\"][-1][0].item())\n",
    "        reduce_count_list.append(val[\"reduction\"][-1][0].item())\n",
    "    return normal_count_list, reduce_count_list\n",
    "\n",
    "lora_darts_normal_count, lora_darts_reduce_count = _get_count_list(lora_darts_dict)\n",
    "darts_normal_count, darts_reduce_count = _get_count_list(darts_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18c32986-e21a-4dd1-9e80-d11302f0f43e",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(np.mean(darts_normal_count), np.std(darts_normal_count))\n",
    "print(np.mean(lora_darts_normal_count), np.std(lora_darts_normal_count))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f1e2dff-aa92-4fd9-93d6-31a7b9498168",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(np.mean(darts_reduce_count), np.std(darts_reduce_count))\n",
    "print(np.mean(lora_darts_reduce_count), np.std(lora_darts_reduce_count))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
